{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-25T12:23:58.495030Z",
     "iopub.status.busy": "2024-09-25T12:23:58.494308Z",
     "iopub.status.idle": "2024-09-25T12:24:03.729522Z",
     "shell.execute_reply": "2024-09-25T12:24:03.728566Z",
     "shell.execute_reply.started": "2024-09-25T12:23:58.494992Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from models.esrpcb import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-25T12:24:03.731452Z",
     "iopub.status.busy": "2024-09-25T12:24:03.730941Z",
     "iopub.status.idle": "2024-09-25T12:24:03.794823Z",
     "shell.execute_reply": "2024-09-25T12:24:03.793839Z",
     "shell.execute_reply.started": "2024-09-25T12:24:03.731417Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tạo Mô hình SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-25T12:24:03.824325Z",
     "iopub.status.busy": "2024-09-25T12:24:03.823331Z",
     "iopub.status.idle": "2024-09-25T12:24:03.833160Z",
     "shell.execute_reply": "2024-09-25T12:24:03.832215Z",
     "shell.execute_reply.started": "2024-09-25T12:24:03.824292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, scale=4, patch_size=96, valid=False):\n",
    "        self.lr_files = sorted(os.listdir(lr_dir))\n",
    "        self.hr_files = sorted(os.listdir(hr_dir))\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.valid = valid\n",
    "        self.scale = scale\n",
    "        self.patch_size = patch_size  # Kích thước patch crop\n",
    "    def __len__(self):\n",
    "        return len(self.lr_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        lr_img = Image.open(os.path.join(self.lr_dir, self.lr_files[idx])).convert('RGB')\n",
    "        hr_img = Image.open(os.path.join(self.hr_dir, self.hr_files[idx])).convert('RGB')\n",
    "        \n",
    "        if self.valid:\n",
    "            def transform_fn(hr_img, lr_img):\n",
    "                # print(lr_img.size)\n",
    "                \n",
    "                lr_img = TF.resize(hr_img, (w // self.scale, h // self.scale), antialias=True)\n",
    "                \n",
    "                lr_img = TF.to_tensor(lr_img)\n",
    "                hr_img = TF.to_tensor(hr_img)\n",
    "        \n",
    "                return lr_img, hr_img\n",
    "\n",
    "        else:\n",
    "            def transform_fn(hr_img, lr_img):\n",
    "                w, h = hr_img.size\n",
    "                top = random.randint(0, h - self.patch_size)\n",
    "                left = random.randint(0, w - self.patch_size)\n",
    "                hr_img = TF.crop(hr_img, top, left, self.patch_size, self.patch_size)\n",
    "\n",
    "                # Crop ảnh LR tương ứng (phải chia tỷ lệ với scale)\n",
    "                w,h = hr_img.size\n",
    "                lr_img = TF.crop(hr_img, (w // self.scale, h // self.scale), antialias=True)\n",
    "            \n",
    "                # Chuyển sang tensor\n",
    "                hr_img = TF.to_tensor(hr_img)\n",
    "                lr_img = TF.to_tensor(lr_img)\n",
    "            \n",
    "                return lr_img, hr_img\n",
    "        lr_img, hr_img = transform_fn(hr_img, lr_img)\n",
    "        return lr_img, hr_img\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tạo Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-25T12:24:03.835254Z",
     "iopub.status.busy": "2024-09-25T12:24:03.834750Z",
     "iopub.status.idle": "2024-09-25T12:24:03.841602Z",
     "shell.execute_reply": "2024-09-25T12:24:03.840600Z",
     "shell.execute_reply.started": "2024-09-25T12:24:03.835180Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Đường dẫn tới bộ dữ liệu\n",
    "\n",
    "# test_hr_dir  = '/kaggle/input/srdataset/sr_data/test/HR'\n",
    "# test_lr_dir  = '/kaggle/input/srdataset/sr_data/test/LR'\n",
    "\n",
    "# print(torch.cuda.memory_allocated())\n",
    "# print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psnr(img1, img2):\n",
    "    mse = torch.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    max_pixel = 1.0\n",
    "    psnr = 20 * torch.log10(max_pixel / torch.sqrt(mse))\n",
    "    return psnr.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
    "\n",
    "def calculate_metrics(img1, img2, max_pixel_value=1.0):\n",
    "    psnr = PeakSignalNoiseRatio(data_range=1.0).to(device)\n",
    "    ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "    psnr_value = psnr(img1, img2)\n",
    "    ssim_value = ssim(img1, img2)\n",
    "    return psnr_value, ssim_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-25T12:33:32.155872Z",
     "iopub.status.busy": "2024-09-25T12:33:32.154884Z",
     "iopub.status.idle": "2024-09-25T14:57:54.133761Z",
     "shell.execute_reply": "2024-09-25T14:57:54.132406Z",
     "shell.execute_reply.started": "2024-09-25T12:33:32.155839Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.amp import autocast, GradScaler\n",
    "from torchsummary import summary\n",
    "import math\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Khởi tạo dataset và dataloader\n",
    "# for scale in [2, 3, 4]:\n",
    "scale = 4\n",
    "batch_size = 16\n",
    "learing_rate = 1e-4\n",
    "num_iterations= 3e5\n",
    "\n",
    "train_lr_dir = 'dataset/train/images'\n",
    "train_hr_dir = 'dataset/train/images'\n",
    "valid_lr_dir = 'dataset/val/images'\n",
    "valid_hr_dir = 'dataset/val/images'\n",
    "train_dataset = ImageDataset(train_lr_dir, train_hr_dir, scale=scale)\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = ImageDataset(valid_lr_dir, valid_hr_dir, scale=scale, valid=True)\n",
    "valid_loader = DataLoader(valid_dataset)\n",
    "\n",
    "\n",
    "\n",
    "sobelsr = ESRPCB(scale_factor = scale, use_sobel = True).to(device)\n",
    "optim_sobel = optim.Adam(sobelsr.parameters(), lr=learing_rate,betas =(0.9, 0.999))\n",
    "scheduler_sobel = optim.lr_scheduler.StepLR(optim_sobel, step_size=10**5, gamma=0.5)\n",
    "\n",
    "cannysr = ESRPCB(scale_factor = scale, use_canny = True).to(device)\n",
    "optim_canny = optim.Adam(cannysr.parameters(), lr=learing_rate,betas =(0.9, 0.999))\n",
    "scheduler_canny = optim.lr_scheduler.StepLR(optim_canny, step_size=10**5, gamma=0.5)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "num_epochs = math.ceil(num_iterations / len(train_loader))\n",
    "\n",
    "best_psnr_sobel = float('-inf')\n",
    "best_psnr_canny = float('-inf')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "losses_sobel = []\n",
    "losses_canny = []\n",
    "avg_psnr_sobel = []\n",
    "avg_psnr_canny = []\n",
    "\n",
    "val_avg_psnr_sobel = []  # Validation PSNR\n",
    "val_avg_psnr_canny = []\n",
    "\n",
    "patience = int(0.25 * num_epochs)\n",
    "epochs_no_improve = 0\n",
    "log_file = open('outputs/train_log/esrpcb.txt', 'a')\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    sobelsr.train()\n",
    "    cannysr.train()\n",
    "\n",
    "    epoch_loss_sobel = 0\n",
    "    psnr_values_sobel = 0\n",
    "    epoch_loss_canny = 0\n",
    "    psnr_values_canny = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Training loop\n",
    "    for (lr_images, hr_images) in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch'):\n",
    "        lr_images = lr_images.cuda()\n",
    "        hr_images = hr_images.cuda()\n",
    "\n",
    "        # Sobel SR training\n",
    "        optim_sobel.zero_grad()  \n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs_sobel = sobelsr(lr_images)\n",
    "            loss_sobel = criterion(outputs_sobel, hr_images)\n",
    "        psnr_sobel = calculate_psnr(outputs_sobel, hr_images)\n",
    "            \n",
    "        scaler.scale(loss_sobel).backward()\n",
    "        scaler.step(optim_sobel)\n",
    "        scaler.update()\n",
    "        scheduler_sobel.step()\n",
    "\n",
    "        # Canny SR training\n",
    "        optim_canny.zero_grad()  \n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs_canny = cannysr(lr_images)\n",
    "            loss_canny = criterion(outputs_canny, hr_images)\n",
    "        psnr_canny = calculate_psnr(outputs_canny, hr_images)\n",
    "\n",
    "        scaler.scale(loss_canny).backward()\n",
    "        scaler.step(optim_canny)\n",
    "        scaler.update()\n",
    "        scheduler_canny.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        epoch_loss_sobel += loss_sobel.item()\n",
    "        psnr_values_sobel += psnr_sobel\n",
    "        epoch_loss_canny += loss_canny.item()\n",
    "        psnr_values_canny += psnr_canny\n",
    "\n",
    "    # Calculate average training metrics\n",
    "    avg_epoch_loss_sobel = epoch_loss_sobel / len(train_loader)\n",
    "    average_psnr_sobel = psnr_values_sobel / len(train_loader)\n",
    "    losses_sobel.append(avg_epoch_loss_sobel)\n",
    "    avg_psnr_sobel.append(average_psnr_sobel)\n",
    "\n",
    "    avg_epoch_loss_canny = epoch_loss_canny / len(train_loader)\n",
    "    average_psnr_canny = psnr_values_canny / len(train_loader)\n",
    "    losses_canny.append(avg_epoch_loss_canny)\n",
    "    avg_psnr_canny.append(average_psnr_canny)\n",
    "\n",
    "    # Validation step\n",
    "    sobelsr.eval()\n",
    "    cannysr.eval()\n",
    "\n",
    "    val_psnr_values_sobel = 0\n",
    "    val_psnr_values_canny = 0\n",
    "\n",
    "    with torch.no_grad():  # No gradients during validation\n",
    "        for (lr_images, hr_images) in tqdm(valid_loader, desc=f'Validation Epoch {epoch+1}/{num_epochs}', unit='batch'):\n",
    "            lr_images = lr_images.cuda()\n",
    "            hr_images = hr_images.cuda()\n",
    "\n",
    "            # Sobel SR validation (no loss, only PSNR)\n",
    "            outputs_sobel = sobelsr(lr_images)\n",
    "            psnr_sobel = calculate_psnr(outputs_sobel, hr_images)\n",
    "\n",
    "            # Canny SR validation (no loss, only PSNR)\n",
    "            outputs_canny = cannysr(lr_images)\n",
    "            psnr_canny = calculate_psnr(outputs_canny, hr_images)\n",
    "\n",
    "            # Update validation PSNR\n",
    "            val_psnr_values_sobel += psnr_sobel\n",
    "            val_psnr_values_canny += psnr_canny\n",
    "\n",
    "    # Calculate average validation PSNR\n",
    "    val_average_psnr_sobel = val_psnr_values_sobel / len(valid_loader)\n",
    "    val_avg_psnr_sobel.append(val_average_psnr_sobel)\n",
    "\n",
    "    val_average_psnr_canny = val_psnr_values_canny / len(valid_loader)\n",
    "    val_avg_psnr_canny.append(val_average_psnr_canny)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Logging results\n",
    "    log_string = (f\"Epoch {epoch+1}/{num_epochs}, Loss sobel: {avg_epoch_loss_sobel:.4f}, \"\n",
    "                f\"Loss canny: {avg_epoch_loss_canny:.4f}, Time training: {end_time - start_time:.4f}s, \"\n",
    "                f\"PSNR sobel: {average_psnr_sobel:.2f} dB, PSNR canny: {average_psnr_canny:.2f} dB, \"\n",
    "                f\"Val PSNR sobel: {val_average_psnr_sobel:.2f} dB, Val PSNR canny: {val_average_psnr_canny:.2f} dB\")\n",
    "    print(log_string)\n",
    "    log_file.write(log_string + '\\n')\n",
    "    log_file.flush()\n",
    "\n",
    "    # Save best models based on validation PSNR\n",
    "    if val_average_psnr_sobel > best_psnr_sobel:\n",
    "        best_psnr_sobel = val_average_psnr_sobel\n",
    "        torch.save(sobelsr.state_dict(), f'outputs/weight_sr/best_esrpcb_sobel.pth')\n",
    "        print(f\"Saved Sobel SR model with PSNR {best_psnr_sobel:.4f}\")\n",
    "        epochs_no_improve=0\n",
    "    \n",
    "\n",
    "    if val_average_psnr_canny > best_psnr_canny:\n",
    "        best_psnr_canny = val_average_psnr_canny\n",
    "        torch.save(cannysr.state_dict(), f'outputs/weight_sr/best_esrpcb_canny.pth')\n",
    "        print(f\"Saved Canny SR model with PSNR {best_psnr_canny:.4f}\")\n",
    "        epochs_no_improve=0\n",
    "    \n",
    "    if (val_average_psnr_sobel < best_psnr_sobel) and (val_average_psnr_canny < best_psnr_canny):\n",
    "        epochs_no_improve+=1\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"PSNR did not improve for {patience} epochs. Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "    # Clear cache and optionally save models at each epoch\n",
    "    save_dir = f'outputs/path/x{scale}'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    torch.save(sobelsr.state_dict(), os.path.join(save_dir, f'esrpcb_sobel_{epoch}.pth'))\n",
    "    torch.save(cannysr.state_dict(), os.path.join(save_dir, f'esrpcb_canny_{epoch}.pth'))\n",
    "        # Close log file after training\n",
    "log_file.close()\n",
    "\n",
    "# Plotting results\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(losses_sobel, label='Sobel SR Loss (Train)')\n",
    "plt.plot(losses_canny, label='Canny SR Loss (Train)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Loss')\n",
    "\n",
    "# Plot PSNR\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(avg_psnr_sobel, label='Sobel SR PSNR (Train)')\n",
    "plt.plot(val_avg_psnr_sobel, label='Sobel SR PSNR (Val)')\n",
    "plt.plot(avg_psnr_canny, label='Canny SR PSNR (Train)')\n",
    "plt.plot(val_avg_psnr_canny, label='Canny SR PSNR (Val)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('PSNR (dB)')\n",
    "plt.legend()\n",
    "plt.title('Average PSNR (Train and Val)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-25T12:24:11.229549Z",
     "iopub.status.idle": "2024-09-25T12:24:11.229851Z",
     "shell.execute_reply": "2024-09-25T12:24:11.229713Z",
     "shell.execute_reply.started": "2024-09-25T12:24:11.229700Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cannysr = cannysr.to(device)\n",
    "sobelsr = sobelsr.to(device)\n",
    "sobelsr.eval()\n",
    "cannysr.eval()\n",
    "val_psnr_values_sobel = 0\n",
    "val_psnr_values_canny = 0\n",
    "val_ssim_values_sobel = 0\n",
    "val_ssim_values_canny = 0\n",
    "torch.cuda.empty_cache()\n",
    "with torch.no_grad():  # No gradients during validation\n",
    "        for (lr_images, hr_images) in tqdm(valid_loader, desc=f'Validation', unit='batch'):\n",
    "                lr_images = lr_images.to(device)\n",
    "                hr_images = hr_images.to(device)\n",
    "\n",
    "                # Sobel SR validation (no loss, only PSNR)\n",
    "                outputs_sobel = sobelsr(lr_images)\n",
    "                psnr_sobel, ssim_sobel = calculate_metrics(outputs_sobel, hr_images)\n",
    "\n",
    "                # Canny SR validation (no loss, only PSNR)\n",
    "                outputs_canny = cannysr(lr_images)\n",
    "                psnr_canny, ssim_canny = calculate_metrics(outputs_canny, hr_images)\n",
    "\n",
    "                # Update validation PSNR\n",
    "                val_psnr_values_sobel += psnr_sobel\n",
    "                val_psnr_values_canny += psnr_canny\n",
    "                val_ssim_values_sobel += ssim_sobel\n",
    "                val_ssim_values_canny += ssim_canny\n",
    "        \n",
    "        # Calculate average validation PSNR & SSIM\n",
    "        val_average_psnr_sobel = val_psnr_values_sobel / len(valid_loader)\n",
    "        val_average_psnr_canny = val_psnr_values_canny / len(valid_loader)\n",
    "        val_average_ssim_sobel = val_ssim_values_sobel / len(valid_loader)\n",
    "        val_average_ssim_canny = val_ssim_values_canny / len(valid_loader)\n",
    "        print(f'esrpcb canny: {val_average_ssim_canny:.4f} / {val_average_psnr_canny:.2f}')\n",
    "        print(f'esrpcb sobel: {val_average_ssim_sobel:.4f} / {val_average_psnr_sobel:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5414450,
     "sourceId": 8989742,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5671931,
     "sourceId": 9356096,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5764801,
     "sourceId": 9478075,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "pcb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
